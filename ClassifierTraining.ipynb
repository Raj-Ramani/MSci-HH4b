{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avoids running on GPU\n",
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "\n",
    "# hep imports\n",
    "# import mplhep as hep\n",
    "# hep.style.use('ATLAS')\n",
    "from utils import load_nnt\n",
    "\n",
    "# standard libraries imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "import uproot\n",
    "import ROOT\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# tensorflow imports\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# import shap to visualise feature importance\n",
    "#import shap\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# import utils.py\n",
    "import utils\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# mc file paths\n",
    "file_path_mc_16 = \"nominal_1.root\"\n",
    "file_path_mc_17 = \"nominal_2.root\"\n",
    "file_path_mc_18 = \"nominal_3.root\"\n",
    "# get data\n",
    "mc16,mc17,mc18=utils.get_data(file_path_mc_16,file_path_mc_17,file_path_mc_18,region='sig',half=\"even\",mc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data files\n",
    "file_path_data_16 = \"data16_NN_100_bootstraps.root\"\n",
    "file_path_data_17 = \"data17_NN_100_bootstraps.root\"\n",
    "file_path_data_18 = \"data18_NN_100_bootstraps.root\"\n",
    "# get data\n",
    "data16,data17,data18=utils.get_data(file_path_data_16,file_path_data_17,file_path_data_18,region='sig',half=\"even\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply masks\n",
    "signal_df = utils.get_data_mask(mc16,mc17,mc18,mask='4b')\n",
    "bkg_df = utils.get_data_mask(data16,data17,data18,mask='2bRW')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## s6 (s5OHE + bkt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# excluding some features that did not agree well in CR\n",
    "features = ['m_hh','X_hh','dEta_hh','X_wt_tag','year_16','year_17','year_18','bkt_0','bkt_1'\n",
    "#             'bkt_lead_jet_pt','bkt_third_lead_jet_pt','pT_h1',\n",
    "#             'cos_theta_star','njets',\n",
    "#            'pt_hh','pT_2','pT_4','eta_i','dRjj_1','dRjj_2','m_min_dj','m_max_dj',\n",
    "#           'pairing_score_1','pairing_score_2',\n",
    "#           'm_h1','E_h1','eta_h1','phi_h1','m_h2','E_h2','pT_h2','eta_h2','phi_h2',\n",
    "#           'm_h1_j1','E_h1_j1','eta_h1_j1','phi_h1_j1',\n",
    "#            'm_h1_j2','E_h1_j2','eta_h1_j2','phi_h1_j2',\n",
    "#           'm_h2_j1','E_h2_j1','eta_h2_j1','phi_h2_j1',\n",
    "#            'm_h2_j2','E_h2_j2','eta_h2_j2','phi_h2_j2','year'\n",
    "           ] \n",
    "\n",
    "# final dataset\n",
    "df_data = pd.concat([signal_df, bkg_df], ignore_index=True)\n",
    "X = df_data[features]\n",
    "y = df_data['class']\n",
    "idx = df_data.index\n",
    "weights = df_data['sample_weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train 70%, valitation 15% and test 15% of the dataset\n",
    "(\n",
    "    X_train,\n",
    "    X_test_validate,\n",
    "    y_train,\n",
    "    y_test_validate,\n",
    "    weights_train,\n",
    "    weights_test_validate,\n",
    "    idx_train,\n",
    "    idx_test_validate,\n",
    ") = train_test_split(X, y, weights, list(idx), test_size=0.3)\n",
    "(\n",
    "    X_test,\n",
    "    X_val,\n",
    "    y_test,\n",
    "    y_val,\n",
    "    weights_test,\n",
    "    weights_val,\n",
    "    idx_test,\n",
    "    idx_val,\n",
    ") = train_test_split(\n",
    "    X_test_validate,\n",
    "    y_test_validate,\n",
    "    weights_test_validate,\n",
    "    idx_test_validate,\n",
    "    test_size=0.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale X\n",
    "scaler = StandardScaler()\n",
    "X_train_sc = scaler.fit_transform(X_train)\n",
    "X_val_sc = scaler.transform(X_val)\n",
    "X_test_sc = scaler.transform(X_test)\n",
    "# convert y to binary class matrix\n",
    "y_train_hot = to_categorical(y_train)\n",
    "y_val_hot = to_categorical(y_val)\n",
    "y_test_hot = to_categorical(y_test)\n",
    "\n",
    "# weights for classes\n",
    "N_bkg_train = weights_train[y_train == 0].sum()\n",
    "N_sig_train = weights_train[y_train==1].sum()\n",
    "# ratio of the weights\n",
    "R = N_bkg_train / N_sig_train\n",
    "# use this ratio for signal events\n",
    "weights_train_R = np.copy(weights_train)\n",
    "weights_train_R[y_train==1] = R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create deep NN model\n",
    "model = Sequential()\n",
    "model.add(Dense(200, input_dim=len(features), activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(200, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose adam optimizer and compile model\n",
    "# note, could have used utils.F1_Score() in metric\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.0005)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    X_train_sc,\n",
    "    y_train_hot,\n",
    "    sample_weight=weights_train_R,\n",
    "    epochs=100,\n",
    "    # early stopping set\n",
    "    callbacks=[EarlyStopping(monitor=\"val_loss\", patience=15, verbose=True)],\n",
    "    batch_size=1000,\n",
    "    # validation data\n",
    "    validation_data=(X_val_sc, y_val_hot, weights_val),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot accuracies during the training \n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions\n",
    "pred_test = model.predict(X_test_sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_positive = pred_test[:,1]\n",
    "pred_negative = pred_test[:,0]\n",
    "# calculate auc\n",
    "auc = roc_auc_score(y_test, pred_positive, sample_weight=weights_test)\n",
    "fpr, tpr, _ = roc_curve(y_test, pred_positive, sample_weight=weights_test)\n",
    "x_fpr = np.linspace(0, 1, 50)\n",
    "y_tpr = np.linspace(0, 1, 50)\n",
    "# plot ROC curve\n",
    "plt.plot(fpr, tpr, ls='-', label = 'NN')\n",
    "plt.plot(x_fpr, y_tpr, ls='--',label = 'random guess')\n",
    "plt.legend()\n",
    "plt.title('AUC: {:.3f}'.format(auc), loc='right')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC curve', loc='left')\n",
    "plt.savefig('ROC.png', dpi = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test_b = np.around(pred_test)[:,1]\n",
    "n_zero = y_test[y_test== 0].shape[0]\n",
    "n_one = y_test[y_test== 1].shape[0]\n",
    "div_arr = np.array([[n_zero,n_one]]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_mat = confusion_matrix(y_test,pred_test_b, labels =[0,1])/div_arr\n",
    "sns.heatmap(conf_mat, annot=True).set(title='Confusion Matrix', xlabel='Predicted', ylabel='Actual');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot NN probability score\n",
    "sm_hh_prob = pred_test[:,1][y_test==1]\n",
    "data_prob = pred_test[:,1][y_test==0]\n",
    "h1, be,_ = plt.hist(sm_hh_prob, bins = 50, histtype='step',label='MC SM HH (probability = positive)', density = True)\n",
    "h2,_,_ = plt.hist(data_prob, bins = be, histtype='step', label = 'data 2b (probability = negative)', density = True)\n",
    "plt.legend()\n",
    "plt.xlabel('NN predicted probability', fontsize=18)\n",
    "plt.ylabel('arb.units', fontsize = 18)\n",
    "plt.ylim(0,2.2)\n",
    "plt.title('NN probability score', fontsize=18)\n",
    "plt.savefig(\"Classifier_hist.png\", dpi = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"./classifier_models/s6_model\")\n",
    "output_dir = \"./classifier_models/\"\n",
    "pickle.dump(scaler, open(output_dir+\"StandardScaler_s6.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## s9 + dRjj_1 + dRjj_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# excluding some features that did not agree well in CR\n",
    "features = ['m_hh','X_hh','dEta_hh','X_wt_tag','year_16','year_17','year_18','bkt_0','bkt_1','pt_hh','m_h1','m_h2','dRjj_1',\n",
    "            'dRjj_2'\n",
    "#             'bkt_lead_jet_pt','bkt_third_lead_jet_pt','pT_h1',\n",
    "#             'cos_theta_star','njets',\n",
    "#            'pt_hh','pT_2','pT_4','eta_i','dRjj_1','dRjj_2','m_min_dj','m_max_dj',\n",
    "#           'pairing_score_1','pairing_score_2',\n",
    "#           'm_h1','E_h1','eta_h1','phi_h1','m_h2','E_h2','pT_h2','eta_h2','phi_h2',\n",
    "#           'm_h1_j1','E_h1_j1','eta_h1_j1','phi_h1_j1',\n",
    "#            'm_h1_j2','E_h1_j2','eta_h1_j2','phi_h1_j2',\n",
    "#           'm_h2_j1','E_h2_j1','eta_h2_j1','phi_h2_j1',\n",
    "#            'm_h2_j2','E_h2_j2','eta_h2_j2','phi_h2_j2','year'\n",
    "           ] \n",
    "\n",
    "# final dataset\n",
    "df_data = pd.concat([signal_df, bkg_df], ignore_index=True)\n",
    "X = df_data[features]\n",
    "y = df_data['class']\n",
    "idx = df_data.index\n",
    "weights = df_data['sample_weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train 70%, valitation 15% and test 15% of the dataset\n",
    "(\n",
    "    X_train,\n",
    "    X_test_validate,\n",
    "    y_train,\n",
    "    y_test_validate,\n",
    "    weights_train,\n",
    "    weights_test_validate,\n",
    "    idx_train,\n",
    "    idx_test_validate,\n",
    ") = train_test_split(X, y, weights, list(idx), test_size=0.3)\n",
    "(\n",
    "    X_test,\n",
    "    X_val,\n",
    "    y_test,\n",
    "    y_val,\n",
    "    weights_test,\n",
    "    weights_val,\n",
    "    idx_test,\n",
    "    idx_val,\n",
    ") = train_test_split(\n",
    "    X_test_validate,\n",
    "    y_test_validate,\n",
    "    weights_test_validate,\n",
    "    idx_test_validate,\n",
    "    test_size=0.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale X\n",
    "scaler = StandardScaler()\n",
    "X_train_sc = scaler.fit_transform(X_train)\n",
    "X_val_sc = scaler.transform(X_val)\n",
    "X_test_sc = scaler.transform(X_test)\n",
    "# convert y to binary class matrix\n",
    "y_train_hot = to_categorical(y_train)\n",
    "y_val_hot = to_categorical(y_val)\n",
    "y_test_hot = to_categorical(y_test)\n",
    "\n",
    "# weights for classes\n",
    "N_bkg_train = weights_train[y_train == 0].sum()\n",
    "N_sig_train = weights_train[y_train==1].sum()\n",
    "# ratio of the weights\n",
    "R = N_bkg_train / N_sig_train\n",
    "# use this ratio for signal events\n",
    "weights_train_R = np.copy(weights_train)\n",
    "weights_train_R[y_train==1] = R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create deep NN model\n",
    "model = Sequential()\n",
    "model.add(Dense(200, input_dim=len(features), activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(200, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose adam optimizer and compile model\n",
    "# note, could have used utils.F1_Score() in metric\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.0005)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    X_train_sc,\n",
    "    y_train_hot,\n",
    "    sample_weight=weights_train_R,\n",
    "    epochs=100,\n",
    "    # early stopping set\n",
    "    callbacks=[EarlyStopping(monitor=\"val_loss\", patience=15, verbose=True)],\n",
    "    batch_size=1000,\n",
    "    # validation data\n",
    "    validation_data=(X_val_sc, y_val_hot, weights_val),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot accuracies during the training \n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions\n",
    "pred_test = model.predict(X_test_sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_positive = pred_test[:,1]\n",
    "pred_negative = pred_test[:,0]\n",
    "# calculate auc\n",
    "auc = roc_auc_score(y_test, pred_positive, sample_weight=weights_test)\n",
    "fpr, tpr, _ = roc_curve(y_test, pred_positive, sample_weight=weights_test)\n",
    "x_fpr = np.linspace(0, 1, 50)\n",
    "y_tpr = np.linspace(0, 1, 50)\n",
    "# plot ROC curve\n",
    "plt.plot(fpr, tpr, ls='-', label = 'NN')\n",
    "plt.plot(x_fpr, y_tpr, ls='--',label = 'random guess')\n",
    "plt.legend()\n",
    "plt.title('AUC: {:.3f}'.format(auc), loc='right')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC curve', loc='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test_b = np.around(pred_test)[:,1]\n",
    "n_zero = y_test[y_test== 0].shape[0]\n",
    "n_one = y_test[y_test== 1].shape[0]\n",
    "div_arr = np.array([[n_zero,n_one]]).T\n",
    "conf_mat = confusion_matrix(y_test,pred_test_b, labels =[0,1])/div_arr\n",
    "sns.heatmap(conf_mat, annot=True).set(title='Confusion Matrix', xlabel='Predicted', ylabel='Actual');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot NN probability score\n",
    "sm_hh_prob = pred_test[:,1][y_test==1]\n",
    "data_prob = pred_test[:,1][y_test==0]\n",
    "h1, be,_ = plt.hist(sm_hh_prob, bins = 50, histtype='step',label='MC SM HH (probability = positive)', density = True)\n",
    "h2,_,_ = plt.hist(data_prob, bins = be, histtype='step', label = 'data 2b (probability = negative)', density = True)\n",
    "plt.legend()\n",
    "plt.xlabel('NN predicted probability', fontsize=18)\n",
    "plt.ylabel('arb.units', fontsize = 18)\n",
    "plt.ylim(0,2.2)\n",
    "plt.title('NN probability score', fontsize=18)\n",
    "plt.savefig(\"Classifier_hist.png\", dpi = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"./classifier_models/s9+dRjj_12_model\")\n",
    "output_dir = \"./classifier_models/\"\n",
    "pickle.dump(scaler, open(output_dir+\"StandardScaler_s9+dRjj_12.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## s9 + dRjj_1 + dRjj_2 + pt_hh + ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['m_hh','X_hh','dEta_hh','X_wt_tag','year_16','year_17','year_18','bkt_0','bkt_1','pt_hh','m_h1','m_h2','dRjj_1',\n",
    "            'dRjj_2','pt_hh','njets', 'E_h1', 'E_h2', 'eta_h1', 'eta_h2', 'phi_h1', 'phi_h2'\n",
    "#             'bkt_lead_jet_pt','bkt_third_lead_jet_pt','pT_h1',\n",
    "#             'cos_theta_star','njets',\n",
    "#            'pt_hh','pT_2','pT_4','eta_i','dRjj_1','dRjj_2','m_min_dj','m_max_dj',\n",
    "#           'pairing_score_1','pairing_score_2',\n",
    "#           'm_h1','E_h1','eta_h1','phi_h1','m_h2','E_h2','pT_h2','eta_h2','phi_h2',\n",
    "#           'm_h1_j1','E_h1_j1','eta_h1_j1','phi_h1_j1',\n",
    "#            'm_h1_j2','E_h1_j2','eta_h1_j2','phi_h1_j2',\n",
    "#           'm_h2_j1','E_h2_j1','eta_h2_j1','phi_h2_j1',\n",
    "#            'm_h2_j2','E_h2_j2','eta_h2_j2','phi_h2_j2','year'\n",
    "           ]\n",
    "\n",
    "# final dataset\n",
    "df_data = pd.concat([signal_df, bkg_df], ignore_index=True)\n",
    "X = df_data[features]\n",
    "y = df_data['class']\n",
    "idx = df_data.index\n",
    "weights = df_data['sample_weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train 70%, valitation 15% and test 15% of the dataset\n",
    "(\n",
    "    X_train,\n",
    "    X_test_validate,\n",
    "    y_train,\n",
    "    y_test_validate,\n",
    "    weights_train,\n",
    "    weights_test_validate,\n",
    "    idx_train,\n",
    "    idx_test_validate,\n",
    ") = train_test_split(X, y, weights, list(idx), test_size=0.3)\n",
    "(\n",
    "    X_test,\n",
    "    X_val,\n",
    "    y_test,\n",
    "    y_val,\n",
    "    weights_test,\n",
    "    weights_val,\n",
    "    idx_test,\n",
    "    idx_val,\n",
    ") = train_test_split(\n",
    "    X_test_validate,\n",
    "    y_test_validate,\n",
    "    weights_test_validate,\n",
    "    idx_test_validate,\n",
    "    test_size=0.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale X\n",
    "scaler = StandardScaler()\n",
    "X_train_sc = scaler.fit_transform(X_train)\n",
    "X_val_sc = scaler.transform(X_val)\n",
    "X_test_sc = scaler.transform(X_test)\n",
    "# convert y to binary class matrix\n",
    "y_train_hot = to_categorical(y_train)\n",
    "y_val_hot = to_categorical(y_val)\n",
    "y_test_hot = to_categorical(y_test)\n",
    "\n",
    "# weights for classes\n",
    "N_bkg_train = weights_train[y_train == 0].sum()\n",
    "N_sig_train = weights_train[y_train==1].sum()\n",
    "# ratio of the weights\n",
    "R = N_bkg_train / N_sig_train\n",
    "# use this ratio for signal events\n",
    "weights_train_R = np.copy(weights_train)\n",
    "weights_train_R[y_train==1] = R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create deep NN model\n",
    "model = Sequential()\n",
    "model.add(Dense(200, input_dim=len(features), activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(200, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose adam optimizer and compile model\n",
    "# note, could have used utils.F1_Score() in metric\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.0005)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    X_train_sc,\n",
    "    y_train_hot,\n",
    "    sample_weight=weights_train_R,\n",
    "    epochs=100,\n",
    "    # early stopping set\n",
    "    callbacks=[EarlyStopping(monitor=\"val_loss\", patience=15, verbose=True)],\n",
    "    batch_size=1000,\n",
    "    # validation data\n",
    "    validation_data=(X_val_sc, y_val_hot, weights_val),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot accuracies during the training \n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions\n",
    "pred_test = model.predict(X_test_sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_positive = pred_test[:,1]\n",
    "pred_negative = pred_test[:,0]\n",
    "# calculate auc\n",
    "auc = roc_auc_score(y_test, pred_positive, sample_weight=weights_test)\n",
    "fpr, tpr, _ = roc_curve(y_test, pred_positive, sample_weight=weights_test)\n",
    "x_fpr = np.linspace(0, 1, 50)\n",
    "y_tpr = np.linspace(0, 1, 50)\n",
    "# plot ROC curve\n",
    "plt.plot(fpr, tpr, ls='-', label = 'NN')\n",
    "plt.plot(x_fpr, y_tpr, ls='--',label = 'random guess')\n",
    "plt.legend()\n",
    "plt.title('AUC: {:.3f}'.format(auc), loc='right')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC curve', loc='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test_b = np.around(pred_test)[:,1]\n",
    "n_zero = y_test[y_test== 0].shape[0]\n",
    "n_one = y_test[y_test== 1].shape[0]\n",
    "div_arr = np.array([[n_zero,n_one]]).T\n",
    "conf_mat = confusion_matrix(y_test,pred_test_b, labels =[0,1])/div_arr\n",
    "sns.heatmap(conf_mat, annot=True).set(title='Confusion Matrix', xlabel='Predicted', ylabel='Actual');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot NN probability score\n",
    "sm_hh_prob = pred_test[:,1][y_test==1]\n",
    "data_prob = pred_test[:,1][y_test==0]\n",
    "h1, be,_ = plt.hist(sm_hh_prob, bins = 50, histtype='step',label='MC SM HH (probability = positive)', density = True)\n",
    "h2,_,_ = plt.hist(data_prob, bins = be, histtype='step', label = 'data 2b (probability = negative)', density = True)\n",
    "plt.legend()\n",
    "plt.xlabel('NN predicted probability', fontsize=18)\n",
    "plt.ylabel('arb.units', fontsize = 18)\n",
    "plt.ylim(0,2.2)\n",
    "plt.title('NN probability score', fontsize=18)\n",
    "plt.savefig(\"Classifier_hist.png\", dpi = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"./classifier_models/s9+dRjj_1+dRjj_2+pt_hh_model\")\n",
    "output_dir = \"./classifier_models/\"\n",
    "pickle.dump(scaler, open(output_dir+\"StandardScaler_s9+dRjj_1+dRjj_2+pt_hh.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## s9 + dRjj_1 + pairing_score_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['m_hh','X_hh','dEta_hh','X_wt_tag','year_16','year_17','year_18','bkt_0','bkt_1','pt_hh','m_h1','m_h2','dRjj_1', \n",
    "            'pairing_score_1'\n",
    "#             'bkt_lead_jet_pt','bkt_third_lead_jet_pt','pT_h1',\n",
    "#             'cos_theta_star','njets',\n",
    "#            'pt_hh','pT_2','pT_4','eta_i','dRjj_1','dRjj_2','m_min_dj','m_max_dj',\n",
    "#           'pairing_score_1','pairing_score_2',\n",
    "#           'm_h1','E_h1','eta_h1','phi_h1','m_h2','E_h2','pT_h2','eta_h2','phi_h2',\n",
    "#           'm_h1_j1','E_h1_j1','eta_h1_j1','phi_h1_j1',\n",
    "#            'm_h1_j2','E_h1_j2','eta_h1_j2','phi_h1_j2',\n",
    "#           'm_h2_j1','E_h2_j1','eta_h2_j1','phi_h2_j1',\n",
    "#            'm_h2_j2','E_h2_j2','eta_h2_j2','phi_h2_j2','year'\n",
    "           ] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final dataset\n",
    "df_data = pd.concat([signal_df, bkg_df], ignore_index=True)\n",
    "X = df_data[features]\n",
    "y = df_data['class']\n",
    "idx = df_data.index\n",
    "weights = df_data['sample_weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train 70%, valitation 15% and test 15% of the dataset\n",
    "(\n",
    "    X_train,\n",
    "    X_test_validate,\n",
    "    y_train,\n",
    "    y_test_validate,\n",
    "    weights_train,\n",
    "    weights_test_validate,\n",
    "    idx_train,\n",
    "    idx_test_validate,\n",
    ") = train_test_split(X, y, weights, list(idx), test_size=0.3)\n",
    "(\n",
    "    X_test,\n",
    "    X_val,\n",
    "    y_test,\n",
    "    y_val,\n",
    "    weights_test,\n",
    "    weights_val,\n",
    "    idx_test,\n",
    "    idx_val,\n",
    ") = train_test_split(\n",
    "    X_test_validate,\n",
    "    y_test_validate,\n",
    "    weights_test_validate,\n",
    "    idx_test_validate,\n",
    "    test_size=0.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale X\n",
    "scaler = StandardScaler()\n",
    "X_train_sc = scaler.fit_transform(X_train)\n",
    "X_val_sc = scaler.transform(X_val)\n",
    "X_test_sc = scaler.transform(X_test)\n",
    "# convert y to binary class matrix\n",
    "y_train_hot = to_categorical(y_train)\n",
    "y_val_hot = to_categorical(y_val)\n",
    "y_test_hot = to_categorical(y_test)\n",
    "\n",
    "# weights for classes\n",
    "N_bkg_train = weights_train[y_train == 0].sum()\n",
    "N_sig_train = weights_train[y_train==1].sum()\n",
    "# ratio of the weights\n",
    "R = N_bkg_train / N_sig_train\n",
    "# use this ratio for signal events\n",
    "weights_train_R = np.copy(weights_train)\n",
    "weights_train_R[y_train==1] = R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create deep NN model\n",
    "model = Sequential()\n",
    "model.add(Dense(200, input_dim=len(features), activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(200, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose adam optimizer and compile model\n",
    "# note, could have used utils.F1_Score() in metric\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=0.0005)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    X_train_sc,\n",
    "    y_train_hot,\n",
    "    sample_weight=weights_train_R,\n",
    "    epochs=100,\n",
    "    # early stopping set\n",
    "    callbacks=[EarlyStopping(monitor=\"val_loss\", patience=15, verbose=True)],\n",
    "    batch_size=1000,\n",
    "    # validation data\n",
    "    validation_data=(X_val_sc, y_val_hot, weights_val),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot accuracies during the training \n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions\n",
    "pred_test = model.predict(X_test_sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_positive = pred_test[:,1]\n",
    "pred_negative = pred_test[:,0]\n",
    "# calculate auc\n",
    "auc = roc_auc_score(y_test, pred_positive, sample_weight=weights_test)\n",
    "fpr, tpr, _ = roc_curve(y_test, pred_positive, sample_weight=weights_test)\n",
    "x_fpr = np.linspace(0, 1, 50)\n",
    "y_tpr = np.linspace(0, 1, 50)\n",
    "# plot ROC curve\n",
    "plt.plot(fpr, tpr, ls='-', label = 'NN')\n",
    "plt.plot(x_fpr, y_tpr, ls='--',label = 'random guess')\n",
    "plt.legend()\n",
    "plt.title('AUC: {:.3f}'.format(auc), loc='right')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC curve', loc='left')\n",
    "plt.savefig('ROC.png', dpi = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test_b = np.around(pred_test)[:,1]\n",
    "n_zero = y_test[y_test== 0].shape[0]\n",
    "n_one = y_test[y_test== 1].shape[0]\n",
    "div_arr = np.array([[n_zero,n_one]]).T\n",
    "conf_mat = confusion_matrix(y_test,pred_test_b, labels =[0,1])/div_arr\n",
    "sns.heatmap(conf_mat, annot=True).set(title='Confusion Matrix', xlabel='Predicted', ylabel='Actual');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot NN probability score\n",
    "sm_hh_prob = pred_test[:,1][y_test==1]\n",
    "data_prob = pred_test[:,1][y_test==0]\n",
    "h1, be,_ = plt.hist(sm_hh_prob, bins = 50, histtype='step',label='MC SM HH (probability = positive)', density = True)\n",
    "h2,_,_ = plt.hist(data_prob, bins = be, histtype='step', label = 'data 2b (probability = negative)', density = True)\n",
    "plt.legend()\n",
    "plt.xlabel('NN predicted probability', fontsize=18)\n",
    "plt.ylabel('arb.units', fontsize = 18)\n",
    "plt.ylim(0,2.2)\n",
    "plt.title('NN probability score', fontsize=18)\n",
    "plt.savefig(\"Classifier_hist.png\", dpi = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"./classifier_models/s9+dRjj_1+pairing_score_1_model\")\n",
    "output_dir = \"./classifier_models/\"\n",
    "pickle.dump(scaler, open(output_dir+\"StandardScaler_s9+dRjj_1+pairing_score_1.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
